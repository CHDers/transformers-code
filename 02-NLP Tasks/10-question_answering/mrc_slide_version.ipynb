{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于截断策略的机器阅读理解任务实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果可以联网，直接使用load_dataset进行加载\n",
    "#datasets = load_dataset(\"cmrc2018\", cache_dir=\"data\")\n",
    "# 如果无法联网，则使用下面的方式加载数据集\n",
    "datasets = DatasetDict.load_from_disk(\"mrc_data\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = datasets[\"train\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples = tokenizer(text=sample_dataset[\"question\"],\n",
    "                               text_pair=sample_dataset[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=384, truncation=\"only_second\", padding=\"max_length\")\n",
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples[\"overflow_to_sample_mapping\"], len(tokenized_examples[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in tokenizer.batch_decode(tokenized_examples[\"input_ids\"][:3]):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_examples[\"offset_mapping\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_examples[\"offset_mapping\"][0], len(tokenized_examples[\"offset_mapping\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, _ in enumerate(sample_mapping):\n",
    "    answer = sample_dataset[\"answers\"][sample_mapping[idx]]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "    # 定位答案在token中的起始位置和结束位置\n",
    "    # 一种策略，我们要拿到context的起始和结束，然后从左右两侧向答案逼近\n",
    "\n",
    "    context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "    context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "\n",
    "    offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
    "    example_ids = []\n",
    "\n",
    "    # 判断答案是否在context中\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id\n",
    "        token_id = context_end\n",
    "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "            token_id -=1\n",
    "        end_token_pos = token_id\n",
    "        example_ids.append()\n",
    "        \n",
    "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)\n",
    "    print(\"token answer decode:\", tokenizer.decode(tokenized_examples[\"input_ids\"][idx][start_token_pos: end_token_pos + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                               text_pair=examples[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=384, truncation=\"only_second\", padding=\"max_length\")\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    for idx, _ in enumerate(sample_mapping):\n",
    "        answer = examples[\"answers\"][sample_mapping[idx]]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        # 定位答案在token中的起始位置和结束位置\n",
    "        # 一种策略，我们要拿到context的起始和结束，然后从左右两侧向答案逼近\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "        offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
    "        # 判断答案是否在context中\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -=1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "        example_ids.append(examples[\"id\"][sample_mapping[idx]])\n",
    "        tokenized_examples[\"offset_mapping\"][idx] = [\n",
    "            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][idx])\n",
    "        ]\n",
    "\n",
    "    \n",
    "    tokenized_examples[\"example_ids\"] = example_ids\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenied_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenied_datasets[\"train\"][\"offset_mapping\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenied_datasets[\"train\"][\"example_ids\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# example 和 feature的映射\n",
    "example_to_feature = collections.defaultdict(list)\n",
    "for idx, example_id in enumerate(tokenied_datasets[\"train\"][\"example_ids\"][:10]):\n",
    "    example_to_feature[example_id].append(idx)\n",
    "example_to_feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 获取模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def get_result(start_logits, end_logits, exmaples, features):\n",
    "\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    # example 和 feature的映射\n",
    "    example_to_feature = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_feature[example_id].append(idx)\n",
    "\n",
    "    # 最优答案候选\n",
    "    n_best = 20\n",
    "    # 最大答案长度\n",
    "    max_answer_length = 30\n",
    "\n",
    "    for example in exmaples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        for feature_idx in example_to_feature[example_id]:\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example_id] = \"\"\n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "\n",
    "    return predictions, references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmrc_eval import evaluate_cmrc\n",
    "\n",
    "def metirc(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenied_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenied_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenied_datasets[\"test\"])\n",
    "    return evaluate_cmrc(p, r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_qa\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 配置Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenied_datasets[\"train\"],\n",
    "    eval_dataset=tokenied_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metirc\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
