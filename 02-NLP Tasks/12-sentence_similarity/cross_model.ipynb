{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本相似度实例"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/yuyao/.cache/huggingface/datasets/json/default-6c4b70959665864d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc628bddc638462e916537e63c238182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ab66deab004812997ef0174da5f38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036a1e8abd724f0cbd47c2a8ff3a9e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/yuyao/.cache/huggingface/datasets/json/default-6c4b70959665864d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./train_pair_1w.json\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': '找一部小时候的动画片', 'sentence2': '求一部小时候的动画片。谢了', 'label': '1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.2)\n",
    "datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a508294406b4c86bb70501cb03be648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d71ff2205ce44bdbb19a348690d9077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "\n",
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = [float(label) for label in examples[\"label\"]]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1062, 4265, 1920, 782, 8024, 1963, 3362, 2769, 1762, 6878, 1168, 2600, 1385, 808, 1184, 6878, 1168, 4640, 2370, 7363, 678, 8024, 6929, 6421, 2582, 720, 1215, 8043, 102, 800, 2697, 6230, 2533, 800, 2190, 6821, 5439, 1928, 2094, 3683, 2190, 800, 1520, 1520, 6820, 779, 8024, 4507, 754, 800, 2190, 6821, 702, 782, 772, 4495, 4638, 3946, 2658, 679, 4881, 2544, 5010, 6629, 3341, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metirc = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(eval_predict):\n",
    "    predictions, labels = eval_predict\n",
    "    predictions = [int(p > 0.5) for p in predictions]\n",
    "    labels = [int(l) for l in labels]\n",
    "    # predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
    "    acc.update(f1)\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 创建TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=True,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./cross_model\\runs\\Jul14_13-44-34_DESKTOP-Q14QE94,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=f1,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=./cross_model,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=32,\n",
       "per_device_train_batch_size=32,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./cross_model,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=3,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_args = TrainingArguments(output_dir=\"./cross_model\",      # 输出文件夹\n",
    "                               per_device_train_batch_size=32,  # 训练时的batch_size\n",
    "                               per_device_eval_batch_size=32,  # 验证时的batch_size\n",
    "                               logging_steps=10,                # log 打印的频率\n",
    "                               evaluation_strategy=\"epoch\",     # 评估策略\n",
    "                               save_strategy=\"epoch\",           # 保存策略\n",
    "                               save_total_limit=3,              # 最大保存数\n",
    "                               learning_rate=2e-5,              # 学习率\n",
    "                               weight_decay=0.01,               # weight_decay\n",
    "                               metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               load_best_model_at_end=True)     # 训练完成后加载最优模型\n",
    "train_args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 创建Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "trainer = Trainer(model=model, \n",
    "                  args=train_args, \n",
    "                  train_dataset=tokenized_datasets[\"train\"], \n",
    "                  eval_dataset=tokenized_datasets[\"test\"], \n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "                  compute_metrics=eval_metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ce8b65fa3946c69d2710e27de4657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2645, 'learning_rate': 1.9733333333333336e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1622, 'learning_rate': 1.9466666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1353, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1558, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1547, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1287, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1432, 'learning_rate': 1.8133333333333335e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1323, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1278, 'learning_rate': 1.76e-05, 'epoch': 0.36}\n",
      "{'loss': 0.1369, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1196, 'learning_rate': 1.706666666666667e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1168, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1455, 'learning_rate': 1.6533333333333333e-05, 'epoch': 0.52}\n",
      "{'loss': 0.1418, 'learning_rate': 1.6266666666666668e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1361, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1151, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1323, 'learning_rate': 1.546666666666667e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0986, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1382, 'learning_rate': 1.4933333333333335e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1246, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1186, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0922, 'learning_rate': 1.4133333333333334e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0979, 'learning_rate': 1.3866666666666669e-05, 'epoch': 0.92}\n",
      "{'loss': 0.1179, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1256, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebe77d24fc74c68b3cbac28963babb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08146345615386963, 'eval_accuracy': 0.8895, 'eval_f1': 0.8550819672131147, 'eval_runtime': 1.8204, 'eval_samples_per_second': 1098.641, 'eval_steps_per_second': 34.607, 'epoch': 1.0}\n",
      "{'loss': 0.0922, 'learning_rate': 1.3066666666666668e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0917, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0988, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0793, 'learning_rate': 1.2266666666666667e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0918, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0811, 'learning_rate': 1.1733333333333335e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0881, 'learning_rate': 1.1466666666666668e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0824, 'learning_rate': 1.1200000000000001e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0685, 'learning_rate': 1.0933333333333334e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0859, 'learning_rate': 1.0666666666666667e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0761, 'learning_rate': 1.04e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0744, 'learning_rate': 1.0133333333333335e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0671, 'learning_rate': 9.866666666666668e-06, 'epoch': 1.52}\n",
      "{'loss': 0.0668, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.56}\n",
      "{'loss': 0.077, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n",
      "{'loss': 0.0769, 'learning_rate': 9.066666666666667e-06, 'epoch': 1.64}\n",
      "{'loss': 0.0626, 'learning_rate': 8.8e-06, 'epoch': 1.68}\n",
      "{'loss': 0.0847, 'learning_rate': 8.533333333333335e-06, 'epoch': 1.72}\n",
      "{'loss': 0.0779, 'learning_rate': 8.266666666666667e-06, 'epoch': 1.76}\n",
      "{'loss': 0.0745, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.8}\n",
      "{'loss': 0.076, 'learning_rate': 7.733333333333334e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0812, 'learning_rate': 7.4666666666666675e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0697, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0709, 'learning_rate': 6.9333333333333344e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0794, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e37d255bdc48778ab547581bbc55ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08298075199127197, 'eval_accuracy': 0.8965, 'eval_f1': 0.8654970760233919, 'eval_runtime': 1.8083, 'eval_samples_per_second': 1105.987, 'eval_steps_per_second': 34.839, 'epoch': 2.0}\n",
      "{'loss': 0.0717, 'learning_rate': 6.4000000000000006e-06, 'epoch': 2.04}\n",
      "{'loss': 0.059, 'learning_rate': 6.133333333333334e-06, 'epoch': 2.08}\n",
      "{'loss': 0.0697, 'learning_rate': 5.8666666666666675e-06, 'epoch': 2.12}\n",
      "{'loss': 0.0578, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.16}\n",
      "{'loss': 0.0716, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.2}\n",
      "{'loss': 0.0573, 'learning_rate': 5.0666666666666676e-06, 'epoch': 2.24}\n",
      "{'loss': 0.0645, 'learning_rate': 4.800000000000001e-06, 'epoch': 2.28}\n",
      "{'loss': 0.0512, 'learning_rate': 4.533333333333334e-06, 'epoch': 2.32}\n",
      "{'loss': 0.0653, 'learning_rate': 4.266666666666668e-06, 'epoch': 2.36}\n",
      "{'loss': 0.0548, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n",
      "{'loss': 0.0453, 'learning_rate': 3.7333333333333337e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0563, 'learning_rate': 3.4666666666666672e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0609, 'learning_rate': 3.2000000000000003e-06, 'epoch': 2.52}\n",
      "{'loss': 0.063, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0427, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0602, 'learning_rate': 2.4000000000000003e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0592, 'learning_rate': 2.133333333333334e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0461, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0679, 'learning_rate': 1.6000000000000001e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0423, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0435, 'learning_rate': 1.066666666666667e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0457, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.88}\n",
      "{'loss': 0.0433, 'learning_rate': 5.333333333333335e-07, 'epoch': 2.92}\n",
      "{'loss': 0.043, 'learning_rate': 2.666666666666667e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0528, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ca1746a9ea41e290533185c4374b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07563769817352295, 'eval_accuracy': 0.911, 'eval_f1': 0.8888888888888888, 'eval_runtime': 1.8058, 'eval_samples_per_second': 1107.519, 'eval_steps_per_second': 34.887, 'epoch': 3.0}\n",
      "{'train_runtime': 86.1781, 'train_samples_per_second': 278.493, 'train_steps_per_second': 8.703, 'train_loss': 0.0897649096250534, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.0897649096250534, metrics={'train_runtime': 86.1781, 'train_samples_per_second': 278.493, 'train_steps_per_second': 8.703, 'train_loss': 0.0897649096250534, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step10 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29c94201b7642eaac6d1f3aa941bedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07563769817352295,\n",
       " 'eval_accuracy': 0.911,\n",
       " 'eval_f1': 0.8888888888888888,\n",
       " 'eval_runtime': 1.8573,\n",
       " 'eval_samples_per_second': 1076.813,\n",
       " 'eval_steps_per_second': 33.92,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step11 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {0: \"不相似\", 1: \"相似\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': '不相似', 'score': 0.054742373526096344}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe({\"text\": \"我喜欢北京\", \"text_pair\": \"天气怎样\"}, function_to_apply=\"none\")\n",
    "result[\"label\"] = \"相似\" if result[\"score\"] > 0.5 else \"不相似\"\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
