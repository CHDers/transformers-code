{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SoftWare\\Anaconda\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer\\\\tokenizer_config.json',\n",
       " './roberta_tokenizer\\\\special_tokens_map.json',\n",
       " './roberta_tokenizer\\\\vocab.txt',\n",
       " './roberta_tokenizer\\\\added_tokens.json',\n",
       " './roberta_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'63': 8381,\n",
       " '##hoo': 12638,\n",
       " '##焼': 17255,\n",
       " '##綜': 18255,\n",
       " '##训': 19435,\n",
       " '##懷': 15812,\n",
       " '牺': 4295,\n",
       " '端': 4999,\n",
       " '纹': 5292,\n",
       " '##ha': 8778,\n",
       " '佞': 870,\n",
       " '斷': 3174,\n",
       " '淡': 3909,\n",
       " '##尔': 15266,\n",
       " '##繕': 18309,\n",
       " '##遏': 19940,\n",
       " '##なりました': 12533,\n",
       " '##毽': 16749,\n",
       " '蠍': 6105,\n",
       " '★★★★★': 10349,\n",
       " 'where': 11703,\n",
       " '##◤': 13618,\n",
       " '##坯': 14848,\n",
       " '##眩': 17757,\n",
       " '##喹': 14671,\n",
       " '##紧': 18222,\n",
       " '##袍': 19208,\n",
       " '##90': 8599,\n",
       " '##成': 15825,\n",
       " '##諜': 19375,\n",
       " '##館': 20688,\n",
       " '幕': 2391,\n",
       " '##遼': 19967,\n",
       " '##恤': 15671,\n",
       " '返': 6819,\n",
       " 'くたさい': 9052,\n",
       " '##笹': 18076,\n",
       " '##际': 20411,\n",
       " '蔽': 5929,\n",
       " '棉': 3469,\n",
       " '媳': 2060,\n",
       " '群': 5408,\n",
       " '##倦': 14015,\n",
       " '##崴': 15368,\n",
       " '##oh': 12355,\n",
       " '覬': 6218,\n",
       " '##扉': 15853,\n",
       " '##浴': 16918,\n",
       " '##笞': 18071,\n",
       " '##陲': 20432,\n",
       " '撥': 3060,\n",
       " '##铭': 20265,\n",
       " '1925': 10087,\n",
       " '##独': 17381,\n",
       " '##韶': 20568,\n",
       " '##歩': 16695,\n",
       " '##2012': 9285,\n",
       " 'tan': 12886,\n",
       " '##un': 8829,\n",
       " '讀': 6364,\n",
       " '237': 10775,\n",
       " '##锈': 20281,\n",
       " '##护': 15901,\n",
       " 'step3': 9434,\n",
       " '65': 8284,\n",
       " '鬢': 7782,\n",
       " 'engine': 11764,\n",
       " '##锁': 20276,\n",
       " '祗': 4863,\n",
       " '##哗': 14574,\n",
       " '祀': 4853,\n",
       " '参': 1346,\n",
       " 'ie': 8469,\n",
       " '##腆': 18627,\n",
       " '蘼': 5986,\n",
       " '##赋': 19659,\n",
       " '##礡': 17902,\n",
       " '擢': 3091,\n",
       " 'december': 9871,\n",
       " '佶': 883,\n",
       " '##钣': 20225,\n",
       " '唇': 1535,\n",
       " '##义': 13778,\n",
       " '525': 11717,\n",
       " '##剎': 14242,\n",
       " '##墮': 14933,\n",
       " '##蠕': 19164,\n",
       " '##do': 8828,\n",
       " '橙': 3581,\n",
       " '##霏': 20511,\n",
       " 'brake': 8550,\n",
       " '##棕': 16530,\n",
       " '培': 1824,\n",
       " '##から': 8825,\n",
       " '##蜚': 19113,\n",
       " '呕': 1445,\n",
       " '弊': 2464,\n",
       " '##犄': 17356,\n",
       " '480': 9482,\n",
       " '鯉': 7805,\n",
       " '##软': 19820,\n",
       " '180': 8420,\n",
       " '##绍': 18362,\n",
       " '##胎': 18579,\n",
       " '厭': 1339,\n",
       " '##曜': 16340,\n",
       " '##痣': 17639,\n",
       " '组': 5299,\n",
       " '##凛': 14180,\n",
       " '296': 11545,\n",
       " 'tvg': 12547,\n",
       " '骋': 7740,\n",
       " '噔': 1683,\n",
       " '汾': 3750,\n",
       " '##z': 8253,\n",
       " '##ヲ': 13702,\n",
       " '##献': 17403,\n",
       " '絶': 5190,\n",
       " '##嗟': 14687,\n",
       " 'jordan': 9229,\n",
       " '繹': 5261,\n",
       " '##ko': 8827,\n",
       " '調': 6310,\n",
       " '##孕': 15154,\n",
       " '餛': 7628,\n",
       " '##滢': 17065,\n",
       " '##柴': 16452,\n",
       " '联': 5468,\n",
       " '##悄': 15689,\n",
       " '##悼': 15713,\n",
       " '##兴': 14126,\n",
       " '##ena': 12000,\n",
       " '扯': 2816,\n",
       " '氢': 3705,\n",
       " '缠': 5362,\n",
       " '鐺': 7138,\n",
       " 'uk': 10375,\n",
       " '##沿': 16841,\n",
       " '化': 1265,\n",
       " '捍': 2932,\n",
       " '##悌': 15692,\n",
       " '槳': 3551,\n",
       " '##れて': 12034,\n",
       " '程': 4923,\n",
       " '囤': 1732,\n",
       " '诠': 6416,\n",
       " 'トラックハック': 12766,\n",
       " '##ⓔ': 13576,\n",
       " '##伊': 13880,\n",
       " '##氣': 16763,\n",
       " '##蔣': 18976,\n",
       " '##鲱': 20894,\n",
       " '莅': 5797,\n",
       " '苁': 5717,\n",
       " 'b1': 9338,\n",
       " '抽': 2853,\n",
       " '##襬': 19259,\n",
       " '##嚼': 14772,\n",
       " '##餾': 20691,\n",
       " '##谪': 19533,\n",
       " '缭': 5369,\n",
       " '##ria': 10159,\n",
       " '##税': 17982,\n",
       " '##馴': 20740,\n",
       " '纜': 5271,\n",
       " '鞍': 7491,\n",
       " 'n1': 11356,\n",
       " 'p1': 11020,\n",
       " '##兮': 14121,\n",
       " '菡': 5834,\n",
       " '凶': 1136,\n",
       " '朮': 3317,\n",
       " '菀': 5820,\n",
       " '魁': 7788,\n",
       " '煅': 4199,\n",
       " '嘻': 1677,\n",
       " '吨': 1417,\n",
       " '##亿': 13840,\n",
       " 'g3': 12493,\n",
       " '##鯨': 20866,\n",
       " '##吱': 14481,\n",
       " 'edge': 9720,\n",
       " '##skip': 12724,\n",
       " '▼': 468,\n",
       " '析': 3358,\n",
       " '##logy': 11121,\n",
       " 'development': 12261,\n",
       " '1955': 9043,\n",
       " '仲': 815,\n",
       " '##007': 12843,\n",
       " '跪': 6661,\n",
       " '級': 5159,\n",
       " '饲': 7654,\n",
       " '##ument': 11867,\n",
       " '##哧': 14579,\n",
       " '##梵': 16521,\n",
       " '##简': 18099,\n",
       " '腴': 5590,\n",
       " '##飢': 20666,\n",
       " '炉': 4140,\n",
       " '勧': 1250,\n",
       " '##°c': 9953,\n",
       " '##严': 13755,\n",
       " 'c3': 11829,\n",
       " '翡': 5429,\n",
       " '##摈': 16089,\n",
       " '##痼': 17650,\n",
       " '棒': 3472,\n",
       " '##よって': 12957,\n",
       " '##豪': 19555,\n",
       " '##⑨': 13564,\n",
       " '凈': 1116,\n",
       " '娆': 2018,\n",
       " '痒': 4573,\n",
       " 'mandy': 9518,\n",
       " '##囿': 14803,\n",
       " '##lie': 10158,\n",
       " 'research': 10018,\n",
       " 'popular': 11880,\n",
       " '##如': 15020,\n",
       " '##妨': 15038,\n",
       " '##媲': 15116,\n",
       " '##オ': 11238,\n",
       " '##迎': 19873,\n",
       " '朗': 3306,\n",
       " '##next': 12397,\n",
       " '##顔': 20600,\n",
       " '##髮': 20830,\n",
       " 'を': 584,\n",
       " 'canmake': 13298,\n",
       " '##滲': 17073,\n",
       " '##改': 16178,\n",
       " '區': 1281,\n",
       " '##fork': 10618,\n",
       " '恋': 2605,\n",
       " '故': 3125,\n",
       " '##獎': 17411,\n",
       " '##僵': 14075,\n",
       " '##争': 13808,\n",
       " '纯': 5283,\n",
       " '飄': 7597,\n",
       " '烤': 4171,\n",
       " 'kg': 9515,\n",
       " '##楽': 16575,\n",
       " '##炳': 17211,\n",
       " '##鞏': 20549,\n",
       " '滁': 3991,\n",
       " '牴': 4292,\n",
       " '昂': 3203,\n",
       " '##北': 14323,\n",
       " '##颐': 20630,\n",
       " '##宴': 15212,\n",
       " 'atm': 8366,\n",
       " '##２': 8929,\n",
       " '##僭': 14072,\n",
       " '##e': 8154,\n",
       " '绩': 5327,\n",
       " '##栄': 16458,\n",
       " '叻': 1387,\n",
       " '1972': 9027,\n",
       " '229': 10608,\n",
       " '##緑': 18275,\n",
       " '圆': 1749,\n",
       " '##chi': 10525,\n",
       " '袱': 6160,\n",
       " '##hur': 13190,\n",
       " '##香': 20733,\n",
       " '詡': 6272,\n",
       " '##21': 8650,\n",
       " '鍥': 7105,\n",
       " '##蜥': 19117,\n",
       " '†': 339,\n",
       " '樹': 3572,\n",
       " '铄': 7191,\n",
       " '血': 6117,\n",
       " 'wei': 11875,\n",
       " '##嬉': 15137,\n",
       " '##～': 8694,\n",
       " '##憊': 15784,\n",
       " '##焯': 17251,\n",
       " '##熠': 17283,\n",
       " 'turbo': 12325,\n",
       " '##コメント': 13301,\n",
       " '##酋': 20036,\n",
       " '翹': 5435,\n",
       " '鹉': 7902,\n",
       " '7500': 11271,\n",
       " '1918': 10106,\n",
       " '445': 12834,\n",
       " 'moba': 11565,\n",
       " '##砰': 17843,\n",
       " 'da': 10005,\n",
       " '133': 9246,\n",
       " '錮': 7096,\n",
       " '213': 10431,\n",
       " '醚': 7009,\n",
       " 'っ': 553,\n",
       " '##烤': 17228,\n",
       " '茲': 5760,\n",
       " '蝈': 6067,\n",
       " '##沅': 16811,\n",
       " '咱': 1493,\n",
       " '240': 8821,\n",
       " '桶': 3446,\n",
       " '##墒': 14923,\n",
       " 'tag': 8801,\n",
       " '锌': 7227,\n",
       " '##鈉': 20099,\n",
       " '縛': 5236,\n",
       " '##繡': 18312,\n",
       " '257': 11027,\n",
       " '##愕': 15750,\n",
       " '静': 7474,\n",
       " '##淹': 16979,\n",
       " '##91': 9332,\n",
       " '##new': 10654,\n",
       " '##谒': 19514,\n",
       " '贲': 6584,\n",
       " '##縛': 18293,\n",
       " '##誤': 19356,\n",
       " '硕': 4798,\n",
       " '##〈': 13650,\n",
       " '戶': 2786,\n",
       " '脣': 5560,\n",
       " '二': 753,\n",
       " '##甾': 17572,\n",
       " '##垩': 14863,\n",
       " '背': 5520,\n",
       " 'ت': 264,\n",
       " '鹑': 7906,\n",
       " '##へ': 11320,\n",
       " '##拳': 15948,\n",
       " '﹗': 8006,\n",
       " 'ب': 262,\n",
       " '##會': 16355,\n",
       " '稟': 4931,\n",
       " '##name': 10509,\n",
       " '##涅': 16922,\n",
       " '鑿': 7150,\n",
       " '##狼': 17388,\n",
       " 'テ': 612,\n",
       " '##04': 9099,\n",
       " '壹': 1902,\n",
       " 'cpi': 9511,\n",
       " '##cake': 12814,\n",
       " '梦': 3457,\n",
       " '涉': 3868,\n",
       " '绛': 5316,\n",
       " '##id': 8601,\n",
       " '仓': 797,\n",
       " '謐': 6337,\n",
       " '涞': 3877,\n",
       " 'van': 9933,\n",
       " '##ques': 12503,\n",
       " '易': 3211,\n",
       " 'in': 8217,\n",
       " '##参': 14403,\n",
       " 'jack': 9850,\n",
       " '##撥': 16117,\n",
       " '2gb': 11380,\n",
       " '##眦': 17755,\n",
       " '篁': 5060,\n",
       " '##嬴': 15145,\n",
       " '婊': 2040,\n",
       " '##觎': 19289,\n",
       " '##77': 8959,\n",
       " '##雅': 20471,\n",
       " '##筲': 18094,\n",
       " 'σ': 226,\n",
       " '##森': 16538,\n",
       " '##ws': 10402,\n",
       " '##the': 11334,\n",
       " '徳': 2545,\n",
       " '##八': 14118,\n",
       " '师': 2360,\n",
       " '术': 3318,\n",
       " '##顱': 20607,\n",
       " '畹': 4536,\n",
       " '2010': 8166,\n",
       " '逢': 6864,\n",
       " '##τ': 13397,\n",
       " '##嫔': 15125,\n",
       " 'l2': 12879,\n",
       " '楂': 3500,\n",
       " '鎚': 7117,\n",
       " '##签': 18098,\n",
       " '##缺': 18432,\n",
       " '婆': 2038,\n",
       " '##tte': 9786,\n",
       " '##のは': 10093,\n",
       " '##✿': 13640,\n",
       " '葭': 5874,\n",
       " '##嬿': 15148,\n",
       " '##封': 15253,\n",
       " '##添': 16981,\n",
       " '##瘍': 17655,\n",
       " '##腑': 18634,\n",
       " '##蠹': 19172,\n",
       " '##ｄ': 9835,\n",
       " '##醬': 20073,\n",
       " '##鬼': 20844,\n",
       " '##麝': 20985,\n",
       " '##82': 9605,\n",
       " '##戏': 15824,\n",
       " '1～2': 10776,\n",
       " '戌': 2764,\n",
       " '蜇': 6047,\n",
       " '桩': 3445,\n",
       " '膛': 5605,\n",
       " '諗': 6317,\n",
       " '##畴': 17590,\n",
       " '熙': 4224,\n",
       " 'wilson': 12348,\n",
       " '呸': 1459,\n",
       " '##萌': 18903,\n",
       " '#': 108,\n",
       " '##兑': 14107,\n",
       " '鍵': 7107,\n",
       " '##培': 14881,\n",
       " '##寺': 15248,\n",
       " '##te': 8299,\n",
       " '##耄': 18498,\n",
       " '##馗': 20732,\n",
       " '##drive': 13115,\n",
       " '涟': 3878,\n",
       " '##澤': 17132,\n",
       " '##縫': 18296,\n",
       " '##fo': 10261,\n",
       " '##千': 14340,\n",
       " '##嘘': 14713,\n",
       " '啖': 1561,\n",
       " 'ゅ': 574,\n",
       " '囗': 1722,\n",
       " '##ral': 10482,\n",
       " '##無': 17249,\n",
       " '拌': 2863,\n",
       " '抬': 2848,\n",
       " '272': 11281,\n",
       " '運': 6880,\n",
       " '##ye': 10522,\n",
       " '卑': 1292,\n",
       " '##遣': 19954,\n",
       " '##脱': 18621,\n",
       " '謾': 6347,\n",
       " '卯': 1312,\n",
       " '別': 1162,\n",
       " '攻': 3122,\n",
       " '##擡': 16147,\n",
       " '##诃': 19453,\n",
       " '槓': 3544,\n",
       " '##抒': 15887,\n",
       " '##蕃': 18988,\n",
       " '##良': 18736,\n",
       " '焦': 4193,\n",
       " 'society': 11573,\n",
       " '##隼': 20465,\n",
       " '##鮪': 20859,\n",
       " '##嗜': 14685,\n",
       " 'oo': 11383,\n",
       " '##泣': 16855,\n",
       " '##鎊': 20168,\n",
       " '侵': 909,\n",
       " '##琲': 17488,\n",
       " '##驚': 20768,\n",
       " '##道': 19944,\n",
       " '礫': 4848,\n",
       " '瘙': 4601,\n",
       " '##ˊ': 13373,\n",
       " '##茶': 18820,\n",
       " '煞': 4208,\n",
       " '峻': 2295,\n",
       " '##ible': 12413,\n",
       " 'official': 12863,\n",
       " '##壑': 14942,\n",
       " '滌': 3997,\n",
       " 'adobe': 8864,\n",
       " '##歲': 16698,\n",
       " 'pinterest': 8379,\n",
       " '##爾': 17330,\n",
       " '豢': 6497,\n",
       " '##粄': 18162,\n",
       " '闢': 7304,\n",
       " 'robert': 9441,\n",
       " '2200': 10191,\n",
       " '##髦': 20828,\n",
       " '跹': 6666,\n",
       " 'sotheby': 12665,\n",
       " '玑': 4375,\n",
       " '##源': 17032,\n",
       " '##nts': 12585,\n",
       " '芜': 5697,\n",
       " '48': 8214,\n",
       " '1976': 8911,\n",
       " '##鳥': 20909,\n",
       " '鐘': 7132,\n",
       " '##link': 9989,\n",
       " '颛': 7581,\n",
       " '尔': 2209,\n",
       " '##瞄': 17787,\n",
       " '震': 7448,\n",
       " '190': 8979,\n",
       " '##険': 20438,\n",
       " '腩': 5583,\n",
       " 'list': 9691,\n",
       " '缮': 5370,\n",
       " '禮': 4891,\n",
       " '##rdon': 12702,\n",
       " '##憔': 15789,\n",
       " '极': 3353,\n",
       " '闕': 7299,\n",
       " '##瑗': 17500,\n",
       " '對': 2205,\n",
       " '曝': 3284,\n",
       " '02': 8150,\n",
       " '姣': 2004,\n",
       " '烙': 4168,\n",
       " 'pierre': 8744,\n",
       " '215': 10082,\n",
       " '##煥': 17267,\n",
       " '##钊': 20210,\n",
       " '諺': 6329,\n",
       " '##苞': 18788,\n",
       " 'gc': 12366,\n",
       " '脸': 5567,\n",
       " '350': 8612,\n",
       " '##59': 9632,\n",
       " '##倾': 14024,\n",
       " 'museum': 10553,\n",
       " '百': 4636,\n",
       " '##壩': 14950,\n",
       " '##600': 10346,\n",
       " '##傚': 14049,\n",
       " '##麴': 20990,\n",
       " 'sorry': 12758,\n",
       " '嗓': 1624,\n",
       " '##恁': 15659,\n",
       " '拆': 2858,\n",
       " '惚': 2666,\n",
       " '##軸': 19786,\n",
       " '動': 1240,\n",
       " '##ould': 11734,\n",
       " '##ィ': 12403,\n",
       " '##嫁': 15120,\n",
       " '擬': 3093,\n",
       " '##螯': 19145,\n",
       " '[unused77]': 77,\n",
       " '##伕': 13886,\n",
       " '##制': 14226,\n",
       " '赋': 6602,\n",
       " '##哨': 14580,\n",
       " '##清': 16983,\n",
       " '便': 912,\n",
       " '邹': 6941,\n",
       " '##弾': 15546,\n",
       " '##墳': 14934,\n",
       " '党': 1054,\n",
       " '##仟': 13863,\n",
       " '烹': 4181,\n",
       " 'mod': 9650,\n",
       " '##灵': 17187,\n",
       " '##羨': 18468,\n",
       " '3t': 12338,\n",
       " '##挟': 15968,\n",
       " '##裸': 19237,\n",
       " '败': 6571,\n",
       " '閃': 7272,\n",
       " '392': 12254,\n",
       " '##mail': 10984,\n",
       " '最': 3297,\n",
       " '藩': 5974,\n",
       " '##ｘ': 13047,\n",
       " '畏': 4519,\n",
       " '又': 1348,\n",
       " 'the': 8174,\n",
       " '得': 2533,\n",
       " 'く': 543,\n",
       " '##51': 9216,\n",
       " '278': 11191,\n",
       " '##θ': 13386,\n",
       " '吠': 1413,\n",
       " '##峭': 15347,\n",
       " '开': 2458,\n",
       " '##祷': 17933,\n",
       " '裱': 6177,\n",
       " '玺': 4389,\n",
       " '156': 9508,\n",
       " 'vogue': 10137,\n",
       " '##ه': 13438,\n",
       " '##曹': 16350,\n",
       " '##渡': 16998,\n",
       " '##谚': 19521,\n",
       " '##鍊': 20158,\n",
       " '7': 128,\n",
       " '##唐': 14595,\n",
       " '##盎': 17718,\n",
       " 'delete': 12845,\n",
       " '##炔': 17201,\n",
       " '##辯': 19857,\n",
       " '幌': 2389,\n",
       " '淺': 3923,\n",
       " '##箭': 18112,\n",
       " '瘦': 4607,\n",
       " 'vdc': 10888,\n",
       " '##隧': 20457,\n",
       " '譽': 6363,\n",
       " '##娃': 15072,\n",
       " '棘': 3475,\n",
       " 'sync': 13193,\n",
       " '##郴': 20016,\n",
       " 'thomas': 10010,\n",
       " 'lte': 8987,\n",
       " '##av': 9876,\n",
       " '柔': 3382,\n",
       " 'に': 558,\n",
       " '彥': 2503,\n",
       " '胀': 5515,\n",
       " '##鍍': 20160,\n",
       " '##する': 9840,\n",
       " '诗': 6408,\n",
       " '試': 6275,\n",
       " '瑗': 4443,\n",
       " '隅': 7383,\n",
       " '并': 2400,\n",
       " '1080': 10680,\n",
       " '##锐': 20286,\n",
       " '##簍': 18136,\n",
       " '##饗': 20700,\n",
       " '锏': 7228,\n",
       " '僥': 1013,\n",
       " '##關': 20359,\n",
       " '##駝': 20749,\n",
       " '凿': 1142,\n",
       " '毛': 3688,\n",
       " '##18': 8662,\n",
       " 'ford': 10296,\n",
       " '210': 9083,\n",
       " '##葬': 18930,\n",
       " '饱': 7653,\n",
       " '##莎': 18858,\n",
       " '璇': 4462,\n",
       " '##ury': 11117,\n",
       " '##をお': 11236,\n",
       " '##熨': 17284,\n",
       " '##乾': 13803,\n",
       " '##鹳': 20974,\n",
       " '諭': 6323,\n",
       " '##慢': 15771,\n",
       " '##沭': 16831,\n",
       " 'ba': 10322,\n",
       " 'ㄤ': 662,\n",
       " '瞌': 4734,\n",
       " '满': 4007,\n",
       " '喎': 1593,\n",
       " '曲': 3289,\n",
       " '開': 7274,\n",
       " '##』': 13657,\n",
       " '##槤': 16605,\n",
       " '##ф': 13419,\n",
       " '##服': 16359,\n",
       " '璐': 4466,\n",
       " '##漂': 17080,\n",
       " '１３': 12911,\n",
       " 'nike': 8702,\n",
       " '##ett': 13151,\n",
       " '##襟': 19256,\n",
       " '##连': 19882,\n",
       " '##鴻': 20919,\n",
       " '##oz': 13102,\n",
       " '1b': 12217,\n",
       " '##ﾝ': 21119,\n",
       " 'non': 12002,\n",
       " '##gs': 9726,\n",
       " '##穀': 18001,\n",
       " '##nge': 10983,\n",
       " '##屄': 15289,\n",
       " '##怀': 15634,\n",
       " '##錠': 20148,\n",
       " '獄': 4352,\n",
       " 'enter': 11086,\n",
       " '俠': 927,\n",
       " '沌': 3757,\n",
       " '1929': 9792,\n",
       " '##缨': 18423,\n",
       " '綿': 5214,\n",
       " '鯽': 7811,\n",
       " '##喟': 14658,\n",
       " '##糍': 18186,\n",
       " '忒': 2560,\n",
       " '##嘿': 14735,\n",
       " '斫': 3169,\n",
       " '##儀': 14078,\n",
       " '##祿': 17936,\n",
       " '998': 13180,\n",
       " 'メ': 627,\n",
       " '沧': 3771,\n",
       " '蓿': 5911,\n",
       " '##ily': 11779,\n",
       " '讯': 6380,\n",
       " '##谩': 19532,\n",
       " '鋰': 7084,\n",
       " '##腐': 18633,\n",
       " '冯': 1101,\n",
       " '390': 10370,\n",
       " '##例': 13948,\n",
       " '##gm': 11390,\n",
       " '##嘴': 14730,\n",
       " '樯': 3568,\n",
       " '##碱': 17879,\n",
       " '##断': 16228,\n",
       " '噗': 1684,\n",
       " 'fm': 9079,\n",
       " '##ode': 10260,\n",
       " '鈎': 7044,\n",
       " '##浒': 16905,\n",
       " '揃': 2984,\n",
       " '喺': 1615,\n",
       " 'ｐ': 8066,\n",
       " '##淚': 16964,\n",
       " '##ネ': 13691,\n",
       " '##跌': 19706,\n",
       " '##po': 9401,\n",
       " '##ani': 13154,\n",
       " '注': 3800,\n",
       " '##紜': 18218,\n",
       " '淳': 3919,\n",
       " '##瀑': 17162,\n",
       " '##謬': 19402,\n",
       " '纾': 5295,\n",
       " '凝': 1125,\n",
       " '髒': 7766,\n",
       " 'way': 10590,\n",
       " '##浚': 16909,\n",
       " '丁': 672,\n",
       " '佥': 876,\n",
       " '##硼': 17861,\n",
       " '##括': 15943,\n",
       " '霈': 7449,\n",
       " '喚': 1598,\n",
       " '##艳': 18740,\n",
       " '##熬': 17285,\n",
       " '够': 1916,\n",
       " '博': 1300,\n",
       " '膜': 5606,\n",
       " 'さん': 10533,\n",
       " '##苫': 18794,\n",
       " '閔': 7280,\n",
       " '叮': 1376,\n",
       " '湃': 3955,\n",
       " 'ヲ': 641,\n",
       " '1280': 10350,\n",
       " '##携': 16082,\n",
       " '##2016': 8386,\n",
       " '2021': 9960,\n",
       " '##張': 15541,\n",
       " '##识': 19456,\n",
       " '##ㄥ': 13721,\n",
       " '貼': 6528,\n",
       " '韌': 7501,\n",
       " 'msi': 12560,\n",
       " '驕': 7709,\n",
       " '##囲': 14796,\n",
       " '##饋': 20694,\n",
       " '勛': 1244,\n",
       " '##舛': 18714,\n",
       " '##抟': 15897,\n",
       " '##綺': 18267,\n",
       " '2005': 8232,\n",
       " '##辖': 19842,\n",
       " '停': 977,\n",
       " '财': 6568,\n",
       " '##魁': 20845,\n",
       " '##箝': 18108,\n",
       " '上': 677,\n",
       " '兵': 1070,\n",
       " 'ca': 8850,\n",
       " '##泽': 16870,\n",
       " '1964': 9129,\n",
       " '135': 8908,\n",
       " '鼎': 7959,\n",
       " '沫': 3773,\n",
       " 'sep': 9463,\n",
       " '##bu': 11381,\n",
       " '174': 10134,\n",
       " '鄙': 6968,\n",
       " '150': 8269,\n",
       " 'あ': 534,\n",
       " '##mann': 10750,\n",
       " '镖': 7259,\n",
       " 'camp': 12275,\n",
       " 'spf': 12598,\n",
       " '皮': 4649,\n",
       " '溜': 3977,\n",
       " '##奧': 15010,\n",
       " 'ko': 10368,\n",
       " 'today': 11262,\n",
       " '##誌': 19347,\n",
       " '涵': 3891,\n",
       " '##钉': 20209,\n",
       " '1930': 9275,\n",
       " '伐': 827,\n",
       " '##汨': 16797,\n",
       " '##烩': 17232,\n",
       " '楊': 3501,\n",
       " '紅': 5148,\n",
       " 'bloomberg': 10313,\n",
       " '##佃': 13908,\n",
       " '249': 10705,\n",
       " '界': 4518,\n",
       " 'fairmont': 12208,\n",
       " '##71': 9097,\n",
       " '##莒': 18859,\n",
       " '##衅': 19176,\n",
       " 'ww': 13279,\n",
       " 'contextlink': 11780,\n",
       " '稠': 4932,\n",
       " '##bc': 9242,\n",
       " '[unused43]': 43,\n",
       " '鑾': 7149,\n",
       " '##奸': 15017,\n",
       " '美': 5401,\n",
       " '##吨': 14474,\n",
       " '##亩': 13831,\n",
       " '##仄': 13844,\n",
       " '##io': 8652,\n",
       " '財': 6512,\n",
       " 'null': 12083,\n",
       " '##富': 15225,\n",
       " '葬': 5873,\n",
       " 'homemesh': 9167,\n",
       " '恸': 2627,\n",
       " '##ｊ': 21097,\n",
       " '憶': 2741,\n",
       " '理': 4415,\n",
       " 'jul': 9618,\n",
       " '碚': 4815,\n",
       " '憑': 2731,\n",
       " '##ｍ': 9295,\n",
       " '葉': 5864,\n",
       " '##惘': 15722,\n",
       " '桥': 3441,\n",
       " '药': 5790,\n",
       " 'eyes': 12909,\n",
       " '##シャンルの': 11947,\n",
       " '##呤': 14508,\n",
       " 'pos': 9854,\n",
       " '##tier': 11955,\n",
       " '##怙': 15645,\n",
       " 'du': 10173,\n",
       " '兆': 1042,\n",
       " '虱': 6003,\n",
       " '##┆': 13580,\n",
       " 'vfm': 8597,\n",
       " '##mand': 12131,\n",
       " '1992': 8508,\n",
       " '##撈': 16108,\n",
       " '##渺': 17010,\n",
       " '見': 6210,\n",
       " '浪': 3857,\n",
       " '腌': 5574,\n",
       " '嫂': 2064,\n",
       " '掣': 2969,\n",
       " '宣': 2146,\n",
       " '慑': 2709,\n",
       " '摊': 3033,\n",
       " '遗': 6890,\n",
       " '##息': 15679,\n",
       " '##结': 18367,\n",
       " '##詼': 19343,\n",
       " '襄': 6198,\n",
       " '##譜': 19412,\n",
       " '##阎': 20387,\n",
       " '荆': 5769,\n",
       " '##烦': 17229,\n",
       " '游': 3952,\n",
       " '恻': 2629,\n",
       " '##burg': 12433,\n",
       " '珑': 4400,\n",
       " '矍': 4754,\n",
       " '##lam': 12953,\n",
       " '##导': 15250,\n",
       " '##験': 20756,\n",
       " '碴': 4824,\n",
       " '##阅': 20382,\n",
       " '慘': 2711,\n",
       " '##life': 10359,\n",
       " '##跃': 19702,\n",
       " '⑧': 412,\n",
       " '諦': 6320,\n",
       " '耨': 5454,\n",
       " '##ok': 9185,\n",
       " '##oot': 13187,\n",
       " '鲤': 7834,\n",
       " '##怪': 15654,\n",
       " '1922': 10209,\n",
       " '##鍥': 20162,\n",
       " '晌': 3233,\n",
       " '##擂': 16133,\n",
       " '##掖': 16019,\n",
       " '珞': 4402,\n",
       " '##cm': 8341,\n",
       " '##逻': 19929,\n",
       " 'cos': 10141,\n",
       " '##tta': 11245,\n",
       " '堕': 1834,\n",
       " 'hi': 8913,\n",
       " 'この': 9134,\n",
       " '鵑': 7864,\n",
       " '##爹': 17326,\n",
       " '##曦': 16343,\n",
       " '##勤': 14306,\n",
       " '鎌': 7112,\n",
       " 'cctv': 10099,\n",
       " '樓': 3559,\n",
       " '慎': 2708,\n",
       " '桌': 3430,\n",
       " '##娜': 15082,\n",
       " '##蜓': 19109,\n",
       " '##od': 9308,\n",
       " '馆': 7667,\n",
       " '反': 1353,\n",
       " '##聾': 18539,\n",
       " '##屠': 15305,\n",
       " '腼': 5595,\n",
       " '配': 6981,\n",
       " '20mm': 12714,\n",
       " '##胶': 18597,\n",
       " '##辞': 19848,\n",
       " '##嘭': 14724,\n",
       " '##鄧': 20028,\n",
       " '##絳': 18245,\n",
       " '##意': 15749,\n",
       " '6m': 12485,\n",
       " '嘔': 1653,\n",
       " '监': 4664,\n",
       " '##lus': 11131,\n",
       " '##剌': 14240,\n",
       " '##詛': 19326,\n",
       " '##寨': 15238,\n",
       " '##抨': 15903,\n",
       " '50cm': 11721,\n",
       " '##燴': 17308,\n",
       " '[unused28]': 28,\n",
       " '塵': 1859,\n",
       " '峤': 2285,\n",
       " '##ax': 10704,\n",
       " 'けた': 13182,\n",
       " '倍': 945,\n",
       " '削': 1181,\n",
       " '叱': 1379,\n",
       " '求': 3724,\n",
       " '野': 7029,\n",
       " '800': 8280,\n",
       " 'emma': 12111,\n",
       " '１４': 12164,\n",
       " '##蒿': 18953,\n",
       " '##蝸': 19137,\n",
       " '##х': 13420,\n",
       " '葡': 5868,\n",
       " '铉': 7194,\n",
       " '1991': 8555,\n",
       " '糠': 5137,\n",
       " '##臀': 18675,\n",
       " '##茜': 18809,\n",
       " '##坏': 14833,\n",
       " 'sp': 9716,\n",
       " '芋': 5692,\n",
       " '壤': 1892,\n",
       " '##鍵': 20164,\n",
       " '##遲': 19960,\n",
       " '##嘚': 14714,\n",
       " '##製': 19239,\n",
       " '##a5': 12540,\n",
       " '##蔺': 18983,\n",
       " '腕': 5580,\n",
       " '##fw': 12851,\n",
       " '##浏': 16903,\n",
       " '弗': 2472,\n",
       " '##笨': 18074,\n",
       " 'guestname': 13082,\n",
       " '稚': 4928,\n",
       " '##訪': 19313,\n",
       " '##粗': 18167,\n",
       " '##瘾': 17671,\n",
       " '##鉤': 20119,\n",
       " '儘': 1029,\n",
       " 'hd': 8407,\n",
       " '逾': 6874,\n",
       " '污': 3738,\n",
       " '##评': 19454,\n",
       " 'sidebar': 11212,\n",
       " '╳': 448,\n",
       " '##定': 15194,\n",
       " '##暗': 16323,\n",
       " '秀': 4899,\n",
       " '##荫': 18846,\n",
       " '芙': 5696,\n",
       " 'emc': 12341,\n",
       " '1957': 9088,\n",
       " '##ima': 13028,\n",
       " 'blogtitle': 10055,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2483</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2207</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4638</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2769</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">738</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3300</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3457</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2682</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3300</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3457</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2682</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6443</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6963</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">749</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">679</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6629</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6841</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6852</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3457</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2682</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4638</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2552</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8024</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3683</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3457</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2682</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3315</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6716</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8024</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3291</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1377</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6586</span>,\n",
       "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'token_type_ids'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m2483\u001b[0m, \u001b[1;36m2207\u001b[0m, \u001b[1;36m4638\u001b[0m, \u001b[1;36m2769\u001b[0m, \u001b[1;36m738\u001b[0m, \u001b[1;36m3300\u001b[0m, \u001b[1;36m1920\u001b[0m, \u001b[1;36m3457\u001b[0m, \u001b[1;36m2682\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m3300\u001b[0m, \u001b[1;36m3457\u001b[0m, \u001b[1;36m2682\u001b[0m, \u001b[1;36m6443\u001b[0m, \u001b[1;36m6963\u001b[0m, \u001b[1;36m749\u001b[0m, \u001b[1;36m679\u001b[0m, \u001b[1;36m6629\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\n",
       "            \u001b[1;36m101\u001b[0m,\n",
       "            \u001b[1;36m6841\u001b[0m,\n",
       "            \u001b[1;36m6852\u001b[0m,\n",
       "            \u001b[1;36m3457\u001b[0m,\n",
       "            \u001b[1;36m2682\u001b[0m,\n",
       "            \u001b[1;36m4638\u001b[0m,\n",
       "            \u001b[1;36m2552\u001b[0m,\n",
       "            \u001b[1;36m8024\u001b[0m,\n",
       "            \u001b[1;36m3683\u001b[0m,\n",
       "            \u001b[1;36m3457\u001b[0m,\n",
       "            \u001b[1;36m2682\u001b[0m,\n",
       "            \u001b[1;36m3315\u001b[0m,\n",
       "            \u001b[1;36m6716\u001b[0m,\n",
       "            \u001b[1;36m8024\u001b[0m,\n",
       "            \u001b[1;36m3291\u001b[0m,\n",
       "            \u001b[1;36m1377\u001b[0m,\n",
       "            \u001b[1;36m6586\u001b[0m,\n",
       "            \u001b[1;36m102\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'token_type_ids'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\",\n",
    "        \"有梦想谁都了不起\",\n",
    "        \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 42.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 17.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 363 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 877 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 266 ms\n",
      "Wall time: 80.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 738 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[39m=\u001b[39m slow_tokenizer(sen, return_offsets_mapping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2537\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2538\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2626\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2641\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2642\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2645\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2646\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2647\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2648\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2649\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2650\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2651\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2652\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2653\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2654\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2655\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2656\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2657\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2658\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2659\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2660\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2661\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2662\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2663\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2714\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2715\u001b[0m )\n\u001b[1;32m-> 2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2718\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2720\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2721\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2722\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2723\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2724\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2725\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2726\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2727\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2728\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2729\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2730\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2731\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2732\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2733\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2734\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2736\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8357a6f05b4345baaffbb95f34fed2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuyao\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccfd3477c5c45e894350699ab1fffc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ce5c1fcbb4d10b64c6b766cf99d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='THUDM/chatglm-6b', vocab_size=130344, model_max_length=2048, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatglm_tokenizer\\\\tokenizer_config.json',\n",
       " 'chatglm_tokenizer\\\\special_tokens_map.json',\n",
       " 'chatglm_tokenizer\\\\ice_text.model',\n",
       " 'chatglm_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"chatglm_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
