{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer\\\\tokenizer_config.json',\n",
       " './roberta_tokenizer\\\\special_tokens_map.json',\n",
       " './roberta_tokenizer\\\\vocab.txt',\n",
       " './roberta_tokenizer\\\\added_tokens.json',\n",
       " './roberta_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'湾': 3968,\n",
       " '訴': 6260,\n",
       " '##轶': 19824,\n",
       " '洞': 3822,\n",
       " '￣': 8100,\n",
       " '##劾': 14288,\n",
       " '##care': 11014,\n",
       " 'asia': 8339,\n",
       " '##嗑': 14679,\n",
       " '##鹘': 20965,\n",
       " 'washington': 12262,\n",
       " '##匕': 14321,\n",
       " '##樟': 16619,\n",
       " '癮': 4628,\n",
       " 'day3': 11649,\n",
       " '##宵': 15213,\n",
       " '##弧': 15536,\n",
       " '##do': 8828,\n",
       " '詭': 6279,\n",
       " '3500': 9252,\n",
       " '124': 9377,\n",
       " '##価': 13957,\n",
       " '##玄': 17428,\n",
       " '##積': 18005,\n",
       " '##肝': 18555,\n",
       " '芦': 5701,\n",
       " '##践': 19721,\n",
       " '##014': 11365,\n",
       " '##麝': 20985,\n",
       " '鎳': 7121,\n",
       " '##烁': 17218,\n",
       " '##肴': 18567,\n",
       " '##訟': 19310,\n",
       " '##切': 14204,\n",
       " '##雞': 20487,\n",
       " '慫': 2718,\n",
       " 'rs': 10610,\n",
       " '##付': 13859,\n",
       " '##杜': 16393,\n",
       " '姐': 1995,\n",
       " '##肺': 18568,\n",
       " '節': 5059,\n",
       " '##舸': 18726,\n",
       " '##译': 19463,\n",
       " '##戎': 15823,\n",
       " '##之': 13779,\n",
       " '##荽': 18853,\n",
       " '距': 6655,\n",
       " '##io': 8652,\n",
       " 'nbapop': 12188,\n",
       " '##惱': 15738,\n",
       " '享': 775,\n",
       " '蜢': 6059,\n",
       " '7x24': 12486,\n",
       " '忖': 2561,\n",
       " '瀛': 4109,\n",
       " '##志': 15619,\n",
       " '50': 8145,\n",
       " '##發': 17691,\n",
       " '##苟': 18789,\n",
       " '##害': 15211,\n",
       " '耳': 5455,\n",
       " '晒': 3235,\n",
       " '最': 3297,\n",
       " 'machine': 12252,\n",
       " '##争': 13808,\n",
       " '##搗': 16073,\n",
       " '瘩': 4609,\n",
       " '347': 12936,\n",
       " '##基': 14882,\n",
       " '##業': 16568,\n",
       " '##ことか': 10274,\n",
       " '##便': 13969,\n",
       " 'reader': 11453,\n",
       " '##46': 9340,\n",
       " '醫': 7015,\n",
       " 'eclipse': 11752,\n",
       " '##蜢': 19116,\n",
       " '339': 11863,\n",
       " '##详': 19479,\n",
       " '纱': 5285,\n",
       " '##08': 9153,\n",
       " '##っと': 10475,\n",
       " '晷': 3254,\n",
       " '##≪': 13549,\n",
       " '„': 338,\n",
       " '阱': 7343,\n",
       " '##评': 19454,\n",
       " '##tra': 9808,\n",
       " '##層': 15308,\n",
       " 'になる': 9322,\n",
       " '##曝': 16341,\n",
       " '椋': 3489,\n",
       " '##永': 16776,\n",
       " '迢': 6827,\n",
       " 'テリヘル': 12739,\n",
       " '##脚': 18615,\n",
       " '##烘': 17224,\n",
       " 'iii': 9207,\n",
       " '暧': 3269,\n",
       " '灞': 4123,\n",
       " '珥': 4405,\n",
       " '胞': 5528,\n",
       " '爻': 4271,\n",
       " '殉': 3653,\n",
       " '钗': 7158,\n",
       " 'global': 8878,\n",
       " '##こさいます': 10286,\n",
       " '##欲': 16674,\n",
       " '##田': 17563,\n",
       " '##ape': 11706,\n",
       " '##访': 19450,\n",
       " '49': 8249,\n",
       " 'aspx': 12363,\n",
       " '##邛': 19983,\n",
       " '谱': 6480,\n",
       " '##接': 16027,\n",
       " '鋰': 7084,\n",
       " 'max': 8621,\n",
       " '雙': 7427,\n",
       " '自': 5632,\n",
       " 'rm': 10620,\n",
       " '请': 6435,\n",
       " 'aa': 9563,\n",
       " '##帜': 15426,\n",
       " '✖': 499,\n",
       " '柳': 3394,\n",
       " '##岩': 15329,\n",
       " '询': 6418,\n",
       " '蜕': 6053,\n",
       " '##★': 8664,\n",
       " '##倏': 14003,\n",
       " '##一': 13728,\n",
       " '##イフ': 12692,\n",
       " '济': 3845,\n",
       " '弈': 2463,\n",
       " 'emily': 12384,\n",
       " '##哮': 14584,\n",
       " '##痊': 17628,\n",
       " '##財': 19569,\n",
       " '##卒': 14350,\n",
       " '##爆': 17312,\n",
       " '184': 10116,\n",
       " '##lton': 10377,\n",
       " '##亘': 13817,\n",
       " '##ux': 9549,\n",
       " '膛': 5605,\n",
       " '阳': 7345,\n",
       " '◕': 475,\n",
       " '##well': 9665,\n",
       " '汗': 3731,\n",
       " '##幕': 15448,\n",
       " '##詫': 19334,\n",
       " '呛': 1448,\n",
       " 'sign': 12710,\n",
       " '##迂': 19868,\n",
       " '##刃': 14202,\n",
       " '##р': 13415,\n",
       " '摳': 3042,\n",
       " '咯': 1492,\n",
       " '樺': 3573,\n",
       " '庶': 2433,\n",
       " '锺': 7247,\n",
       " 'gnu': 12367,\n",
       " '唯': 1546,\n",
       " '##ย': 13446,\n",
       " '##栏': 16465,\n",
       " '託': 6249,\n",
       " 'oppo': 8806,\n",
       " '堕': 1834,\n",
       " '##闷': 20372,\n",
       " '閩': 7287,\n",
       " 'fashion': 9522,\n",
       " 'ryan': 11536,\n",
       " '##嘸': 14732,\n",
       " '粄': 5105,\n",
       " '吆': 1393,\n",
       " '餌': 7622,\n",
       " 'p10': 10405,\n",
       " '蹦': 6698,\n",
       " '##练': 18355,\n",
       " '岙': 2268,\n",
       " '珺': 4411,\n",
       " '闕': 7299,\n",
       " '序': 2415,\n",
       " '##扉': 15853,\n",
       " '##龛': 21046,\n",
       " '刎': 1151,\n",
       " '虛': 5995,\n",
       " '##閑': 20334,\n",
       " '博': 1300,\n",
       " '婚': 2042,\n",
       " '锰': 7243,\n",
       " '[PAD]': 0,\n",
       " 'x7': 12049,\n",
       " '##匆': 14317,\n",
       " '##槲': 16607,\n",
       " '##顏': 20599,\n",
       " '##b': 8204,\n",
       " '##婺': 15109,\n",
       " '鈪': 7050,\n",
       " '渲': 3950,\n",
       " '186': 9833,\n",
       " '墙': 1870,\n",
       " '##pl': 12569,\n",
       " '単': 1299,\n",
       " '##mv': 13275,\n",
       " '##寢': 15234,\n",
       " '##炉': 17197,\n",
       " '##见': 19281,\n",
       " '##bu': 11381,\n",
       " '##de': 8510,\n",
       " '260': 9044,\n",
       " '##樂': 16613,\n",
       " '##狱': 17385,\n",
       " '##稷': 17995,\n",
       " 'financial': 12200,\n",
       " '雞': 7430,\n",
       " '##┣': 13585,\n",
       " '##乔': 13787,\n",
       " '##tory': 12608,\n",
       " '##場': 14899,\n",
       " '对': 2190,\n",
       " '巫': 2344,\n",
       " '缜': 5360,\n",
       " '##坷': 14851,\n",
       " 'union': 12161,\n",
       " '##庫': 15487,\n",
       " '##產': 17553,\n",
       " '##骇': 20794,\n",
       " '##飨': 20667,\n",
       " '１６': 12963,\n",
       " '發': 4634,\n",
       " '人': 782,\n",
       " '##椹': 16554,\n",
       " '##还': 19877,\n",
       " '隧': 7400,\n",
       " '395': 12481,\n",
       " '027': 12849,\n",
       " '壶': 1901,\n",
       " '##軸': 19786,\n",
       " '##蟋': 19151,\n",
       " '##讞': 19425,\n",
       " '紺': 5172,\n",
       " '诞': 6414,\n",
       " '驻': 7728,\n",
       " '##織': 18308,\n",
       " '攘': 3106,\n",
       " '##est': 10414,\n",
       " '##汛': 16790,\n",
       " '##鏖': 20180,\n",
       " '諏': 6314,\n",
       " '胧': 5532,\n",
       " '##裔': 19224,\n",
       " '##駅': 20743,\n",
       " '媧': 2058,\n",
       " '278': 11191,\n",
       " '玳': 4387,\n",
       " '蠛': 6108,\n",
       " '霽': 7466,\n",
       " 'thomas': 10010,\n",
       " '##坳': 14849,\n",
       " '響': 7513,\n",
       " '##雾': 20500,\n",
       " '##world': 10120,\n",
       " '146': 9630,\n",
       " '##oka': 12279,\n",
       " '叟': 1362,\n",
       " '嘧': 1665,\n",
       " '##be': 8765,\n",
       " '##cy': 8890,\n",
       " '##弹': 15543,\n",
       " '##扱': 15875,\n",
       " '##躊': 19768,\n",
       " '##錳': 20156,\n",
       " 'frank': 10379,\n",
       " '类': 5102,\n",
       " '硬': 4801,\n",
       " '##/': 13331,\n",
       " '##萼': 18918,\n",
       " '殘': 3659,\n",
       " '##地': 14822,\n",
       " '##芹': 18770,\n",
       " '##═': 13586,\n",
       " '侷': 911,\n",
       " '383': 12510,\n",
       " 'wan': 10686,\n",
       " '##xxx': 12812,\n",
       " '##投': 15889,\n",
       " '##慮': 15776,\n",
       " '##擄': 16134,\n",
       " '##腊': 18629,\n",
       " '劫': 1223,\n",
       " '涸': 3892,\n",
       " '保': 924,\n",
       " '習': 5424,\n",
       " 'coffee': 9706,\n",
       " 'vdc': 10888,\n",
       " '##非': 20535,\n",
       " '梭': 3460,\n",
       " '测': 3844,\n",
       " '籼': 5103,\n",
       " '素': 5162,\n",
       " '##lv': 9084,\n",
       " '##˙': 13377,\n",
       " '##嫲': 15133,\n",
       " '##梯': 16518,\n",
       " '##dium': 12787,\n",
       " '9595': 13043,\n",
       " '##刻': 14231,\n",
       " '##zz': 9409,\n",
       " 'course': 12654,\n",
       " '##ince': 13199,\n",
       " '里': 7027,\n",
       " 'fast': 11934,\n",
       " '##四': 14781,\n",
       " '##擴': 16154,\n",
       " '##觥': 19295,\n",
       " 'dota': 11636,\n",
       " '##礫': 17905,\n",
       " '忙': 2564,\n",
       " '礙': 4844,\n",
       " '##nk': 9705,\n",
       " '##bbs': 12646,\n",
       " '##燄': 17292,\n",
       " '##仟': 13863,\n",
       " '422': 12721,\n",
       " 'sync': 13193,\n",
       " '伐': 827,\n",
       " '癥': 4625,\n",
       " '程': 4923,\n",
       " '辙': 6788,\n",
       " '郜': 6949,\n",
       " '﹗': 8006,\n",
       " '##報': 14898,\n",
       " '##賊': 19595,\n",
       " '##宸': 15215,\n",
       " 'color': 9916,\n",
       " '##胍': 18578,\n",
       " '##諸': 19385,\n",
       " '##腮': 18643,\n",
       " '##嗓': 14681,\n",
       " '鄔': 6967,\n",
       " 'kiss': 11192,\n",
       " '薙': 5954,\n",
       " '跟': 6656,\n",
       " '##顔': 20600,\n",
       " '##麥': 20987,\n",
       " '耍': 5446,\n",
       " '麵': 7934,\n",
       " '兒': 1051,\n",
       " '參': 1347,\n",
       " '##ey': 8603,\n",
       " '##调': 19501,\n",
       " '##0': 8129,\n",
       " '肋': 5490,\n",
       " '##坟': 14841,\n",
       " '##旧': 16248,\n",
       " '##爸': 17325,\n",
       " '##佑': 13916,\n",
       " '##浃': 16895,\n",
       " 'h1': 12333,\n",
       " '##秘': 17965,\n",
       " '族': 3184,\n",
       " '##般': 18720,\n",
       " '拆': 2858,\n",
       " '〉': 516,\n",
       " '桨': 3444,\n",
       " '195': 9996,\n",
       " '##慵': 15779,\n",
       " '##胝': 18584,\n",
       " '税': 4925,\n",
       " 'xa': 12626,\n",
       " '堂': 1828,\n",
       " 'ᅣ': 306,\n",
       " 'h': 150,\n",
       " '##cture': 11235,\n",
       " '绸': 5339,\n",
       " '鍊': 7101,\n",
       " '##lts': 12977,\n",
       " 'rick': 13253,\n",
       " '##讳': 19440,\n",
       " '宣': 2146,\n",
       " 'case': 10474,\n",
       " '##崎': 15358,\n",
       " '##抜': 15895,\n",
       " 'そして': 11812,\n",
       " '▼': 468,\n",
       " '珐': 4399,\n",
       " '##濁': 17138,\n",
       " 'にも': 9328,\n",
       " '##して': 9379,\n",
       " '廁': 2438,\n",
       " '‹': 347,\n",
       " '##hl': 11831,\n",
       " '##stro': 12976,\n",
       " 'step3': 9434,\n",
       " '须': 7557,\n",
       " '##夭': 14981,\n",
       " '##敢': 16197,\n",
       " '351': 12524,\n",
       " '##鳖': 20905,\n",
       " '##對': 15262,\n",
       " '##蝈': 19124,\n",
       " '６': 8034,\n",
       " '搓': 3013,\n",
       " '哎': 1511,\n",
       " '茸': 5764,\n",
       " '##乡': 13797,\n",
       " 'pc': 8295,\n",
       " '##臭': 18691,\n",
       " '戕': 2771,\n",
       " '狭': 4325,\n",
       " '[unused37]': 37,\n",
       " '万': 674,\n",
       " '寮': 2185,\n",
       " 'movie': 11099,\n",
       " '##漓': 17084,\n",
       " '椅': 3488,\n",
       " '[unused54]': 54,\n",
       " '##伏': 13883,\n",
       " '##♀': 13624,\n",
       " '##word': 10184,\n",
       " '##don': 12107,\n",
       " '##劑': 14269,\n",
       " '硫': 4800,\n",
       " '##药': 18847,\n",
       " 'ｸ': 8089,\n",
       " '##气': 16755,\n",
       " '##铺': 20272,\n",
       " '鹭': 7915,\n",
       " '2l': 13191,\n",
       " '葳': 5877,\n",
       " '##禽': 17953,\n",
       " '##兽': 14134,\n",
       " '絡': 5181,\n",
       " '##妹': 15044,\n",
       " '##聚': 18528,\n",
       " '嘢': 1662,\n",
       " '燉': 4237,\n",
       " '皿': 4654,\n",
       " '##谈': 19505,\n",
       " '##区': 14334,\n",
       " '寝': 2173,\n",
       " '兢': 1056,\n",
       " '珠': 4403,\n",
       " '逃': 6845,\n",
       " '丹': 710,\n",
       " '钛': 7160,\n",
       " '##dical': 13168,\n",
       " '##祐': 17917,\n",
       " '鄭': 6972,\n",
       " '紮': 5167,\n",
       " '□': 460,\n",
       " '獎': 4354,\n",
       " 'γ': 212,\n",
       " 'posted': 8486,\n",
       " '琪': 4427,\n",
       " '叶': 1383,\n",
       " '啼': 1582,\n",
       " '##tv': 9171,\n",
       " '##α': 13380,\n",
       " '##出': 14196,\n",
       " '##岐': 15319,\n",
       " '##腴': 18647,\n",
       " '##苛': 18786,\n",
       " '##返': 19876,\n",
       " '##孰': 15172,\n",
       " '舗': 5656,\n",
       " '慵': 2722,\n",
       " '拯': 2889,\n",
       " '##妝': 15033,\n",
       " '柑': 3379,\n",
       " 'jb': 13063,\n",
       " '303': 11068,\n",
       " '##抢': 15900,\n",
       " '##擎': 16140,\n",
       " '##管': 18109,\n",
       " '##ake': 9981,\n",
       " '##篱': 18132,\n",
       " 'studio': 8318,\n",
       " '##谶': 19540,\n",
       " '##题': 20636,\n",
       " '炯': 4153,\n",
       " '##乳': 13802,\n",
       " '嗔': 1625,\n",
       " '##缩': 18424,\n",
       " '##蟀': 19148,\n",
       " '##เ': 13450,\n",
       " '枪': 3366,\n",
       " '巔': 2333,\n",
       " '##氡': 16761,\n",
       " 'william': 9979,\n",
       " 'g': 149,\n",
       " '##擋': 16138,\n",
       " '[unused73]': 73,\n",
       " 'win': 9769,\n",
       " '##脱': 18621,\n",
       " '##葺': 18937,\n",
       " '##导': 15250,\n",
       " '[unused62]': 62,\n",
       " 'lohas': 9757,\n",
       " '盤': 4676,\n",
       " '灏': 4119,\n",
       " 'please': 11744,\n",
       " '##wt': 12271,\n",
       " '##伝': 13891,\n",
       " '鱼': 7824,\n",
       " '##否': 14472,\n",
       " '##竽': 18060,\n",
       " '鬃': 7776,\n",
       " '##薩': 19015,\n",
       " '痔': 4574,\n",
       " '##玫': 17439,\n",
       " '##央': 14982,\n",
       " '谘': 6462,\n",
       " '##誓': 19349,\n",
       " '缮': 5370,\n",
       " '丐': 681,\n",
       " 'lin': 9733,\n",
       " '鷗': 7875,\n",
       " '##决': 14161,\n",
       " '##曲': 16346,\n",
       " '##鄒': 20023,\n",
       " '##苯': 18795,\n",
       " 'hip': 11070,\n",
       " ',': 117,\n",
       " 'ips': 10511,\n",
       " '##へて': 12864,\n",
       " '##檐': 16649,\n",
       " '飄': 7597,\n",
       " '##涙': 16931,\n",
       " '##昔': 16269,\n",
       " '##﹝': 21066,\n",
       " '##银': 20270,\n",
       " '##獻': 17425,\n",
       " '##ged': 11765,\n",
       " '嚕': 1707,\n",
       " '##the': 11334,\n",
       " '縣': 5238,\n",
       " '低': 856,\n",
       " '落': 5862,\n",
       " '路': 6662,\n",
       " '驳': 7722,\n",
       " '矩': 4762,\n",
       " '1905': 10196,\n",
       " 'square': 10897,\n",
       " '笛': 5013,\n",
       " 'ての': 9697,\n",
       " '1111': 11954,\n",
       " '##β': 13381,\n",
       " '攫': 3116,\n",
       " '##歆': 16679,\n",
       " '檢': 3596,\n",
       " '蝈': 6067,\n",
       " 'nt': 8495,\n",
       " '##men': 11839,\n",
       " '寧': 2180,\n",
       " '桐': 3432,\n",
       " 'arm': 9498,\n",
       " '##离': 17952,\n",
       " '##蔥': 18977,\n",
       " '照': 4212,\n",
       " '耷': 5457,\n",
       " 'evernote': 13138,\n",
       " '⑥': 410,\n",
       " '滅': 3994,\n",
       " '##ᄐ': 13467,\n",
       " '鷹': 7877,\n",
       " 'seo': 8475,\n",
       " '##ff': 9049,\n",
       " '##謾': 19404,\n",
       " '栎': 3407,\n",
       " '豁': 6485,\n",
       " '恸': 2627,\n",
       " 'ㆍ': 666,\n",
       " '##困': 14794,\n",
       " '##邨': 19988,\n",
       " '執': 1822,\n",
       " '諳': 6326,\n",
       " '犀': 4297,\n",
       " '領': 7526,\n",
       " '##谔': 19516,\n",
       " '簌': 5078,\n",
       " '##蒹': 18950,\n",
       " '##イン': 10108,\n",
       " '藉': 5964,\n",
       " '##壁': 14937,\n",
       " '置': 5390,\n",
       " '♂': 487,\n",
       " 'н': 245,\n",
       " '##≫': 13550,\n",
       " '##孩': 15168,\n",
       " '果': 3362,\n",
       " '##盜': 17728,\n",
       " '堤': 1837,\n",
       " '燼': 4253,\n",
       " '例': 891,\n",
       " '##ⅲ': 13521,\n",
       " '##咛': 14538,\n",
       " '##拌': 15920,\n",
       " '沦': 3770,\n",
       " '赡': 6616,\n",
       " '先': 1044,\n",
       " 'e2': 12357,\n",
       " '##闢': 20361,\n",
       " '##骤': 20809,\n",
       " '裤': 6175,\n",
       " 'rate': 12597,\n",
       " '##畏': 17576,\n",
       " '毗': 3685,\n",
       " '流': 3837,\n",
       " '##夯': 14983,\n",
       " '##嵘': 15375,\n",
       " '##闺': 20375,\n",
       " 'sunday': 11548,\n",
       " '姆': 1990,\n",
       " '梢': 3456,\n",
       " 'p2p': 8478,\n",
       " '##紀': 18202,\n",
       " '##臧': 18687,\n",
       " '##乐': 13784,\n",
       " '啥': 1567,\n",
       " '206': 9899,\n",
       " '##鉑': 20115,\n",
       " '##獨': 17417,\n",
       " '楣': 3508,\n",
       " '##嫦': 15131,\n",
       " '杯': 3344,\n",
       " '##惫': 15732,\n",
       " '奖': 1946,\n",
       " '蝎': 6070,\n",
       " '険': 7381,\n",
       " '畝': 4524,\n",
       " 'planet': 12269,\n",
       " '##汀': 16779,\n",
       " '##2d': 12675,\n",
       " '劳': 1227,\n",
       " '竹': 5001,\n",
       " '杆': 3327,\n",
       " '饋': 7637,\n",
       " '##串': 13763,\n",
       " '##帝': 15427,\n",
       " '獷': 4365,\n",
       " '##妒': 15028,\n",
       " '##粒': 18165,\n",
       " 'with': 8663,\n",
       " 'wish': 12255,\n",
       " 'mall': 9628,\n",
       " '##絆': 18233,\n",
       " '##添': 16981,\n",
       " '##莴': 18871,\n",
       " '##兜': 14112,\n",
       " '##啃': 14610,\n",
       " '##杈': 16385,\n",
       " '##胞': 18585,\n",
       " '緻': 5232,\n",
       " '##民': 16753,\n",
       " '痱': 4589,\n",
       " '迷': 6837,\n",
       " '靖': 7473,\n",
       " '輾': 6747,\n",
       " '辍': 6780,\n",
       " 'top100': 10124,\n",
       " '##yan': 10777,\n",
       " '觥': 6238,\n",
       " '##{': 13344,\n",
       " '充': 1041,\n",
       " '##養': 20678,\n",
       " '433': 13131,\n",
       " '蕻': 5944,\n",
       " '##裱': 19234,\n",
       " 'joy': 12668,\n",
       " '葉': 5864,\n",
       " '辊': 6779,\n",
       " '##いて': 10042,\n",
       " '##滴': 17074,\n",
       " '₄': 358,\n",
       " 'maria': 12096,\n",
       " '誘': 6294,\n",
       " '軌': 6724,\n",
       " '##懇': 15801,\n",
       " '##倔': 14006,\n",
       " 'vip': 8324,\n",
       " '##甦': 17556,\n",
       " 'gd': 11729,\n",
       " 'ibm': 8699,\n",
       " 'forest': 10889,\n",
       " '##苏': 18779,\n",
       " '##剐': 14243,\n",
       " '##聂': 18519,\n",
       " '##net': 8914,\n",
       " '##嗟': 14687,\n",
       " 'wedding': 10826,\n",
       " '码': 4772,\n",
       " 'ide': 11319,\n",
       " 'てきます': 11397,\n",
       " '篤': 5071,\n",
       " '##満': 17027,\n",
       " '##酿': 20058,\n",
       " '铉': 7194,\n",
       " '跎': 6650,\n",
       " '##彼': 15573,\n",
       " '镯': 7265,\n",
       " '##惰': 15737,\n",
       " '鹞': 7910,\n",
       " '##邝': 19984,\n",
       " '##玟': 17435,\n",
       " '##箋': 18101,\n",
       " '譙': 6353,\n",
       " '婀': 2036,\n",
       " '瀑': 4105,\n",
       " '嚨': 1711,\n",
       " '賣': 6546,\n",
       " '镰': 7266,\n",
       " '##tal': 9917,\n",
       " '啜': 1562,\n",
       " '##ː': 13376,\n",
       " '##礦': 17903,\n",
       " '蒻': 5894,\n",
       " '︱': 7993,\n",
       " '約': 5147,\n",
       " 'dm': 10442,\n",
       " '驯': 7719,\n",
       " '##噻': 14756,\n",
       " '##硫': 17857,\n",
       " '##癥': 17682,\n",
       " '##茴': 18818,\n",
       " '##療': 17672,\n",
       " '##費': 19584,\n",
       " 'u': 163,\n",
       " '顺': 7556,\n",
       " 'rss': 8605,\n",
       " '##周': 14510,\n",
       " '##杭': 16400,\n",
       " '亦': 771,\n",
       " '##鳞': 20908,\n",
       " '##饵': 20713,\n",
       " '们': 812,\n",
       " '敏': 3130,\n",
       " '稗': 4927,\n",
       " '妩': 1982,\n",
       " '燭': 4249,\n",
       " 'まて': 9642,\n",
       " '##淀': 16952,\n",
       " '##約': 18204,\n",
       " '##释': 20082,\n",
       " '##瑛': 17503,\n",
       " '咻': 1497,\n",
       " 'ォ': 596,\n",
       " '碳': 4823,\n",
       " '##‖': 13496,\n",
       " '##疲': 17615,\n",
       " '##曜': 16340,\n",
       " '癞': 4621,\n",
       " 'ᵘ': 336,\n",
       " 'over': 10047,\n",
       " '呢': 1450,\n",
       " '##烨': 17231,\n",
       " '##阱': 20400,\n",
       " '濮': 4095,\n",
       " '睢': 4718,\n",
       " '222': 9918,\n",
       " '##战': 15830,\n",
       " '妳': 1986,\n",
       " '##抄': 15883,\n",
       " '已': 2347,\n",
       " 'chi': 12205,\n",
       " '##皓': 17702,\n",
       " 'jeff': 11147,\n",
       " '##兰': 14122,\n",
       " '##62': 9290,\n",
       " '##儆': 14081,\n",
       " '##留': 17579,\n",
       " '閃': 7272,\n",
       " '##闆': 20350,\n",
       " '柩': 3390,\n",
       " 'から': 8526,\n",
       " '庙': 2422,\n",
       " '烫': 4176,\n",
       " 'june': 8564,\n",
       " 'ettoday': 9485,\n",
       " '巍': 2331,\n",
       " 'uv': 9473,\n",
       " '##朦': 16367,\n",
       " '##滁': 17048,\n",
       " '賴': 6552,\n",
       " '##折': 15892,\n",
       " '蹄': 6686,\n",
       " '##儿': 14093,\n",
       " '##警': 19413,\n",
       " '##酋': 20036,\n",
       " '楹': 3516,\n",
       " 'lte': 8987,\n",
       " '呦': 1452,\n",
       " '1914': 9837,\n",
       " '##妊': 15026,\n",
       " '##揃': 16041,\n",
       " 'tpp': 8855,\n",
       " '##坝': 14839,\n",
       " '##搖': 16072,\n",
       " '##烈': 17221,\n",
       " '况': 1105,\n",
       " '佩': 877,\n",
       " '茨': 5754,\n",
       " 'hero': 11673,\n",
       " '早': 3193,\n",
       " '##听': 14477,\n",
       " 'ᄋ': 297,\n",
       " '##雪': 20491,\n",
       " '##団': 14788,\n",
       " '俺': 939,\n",
       " 'npc': 9811,\n",
       " '尔': 2209,\n",
       " '##氤': 16764,\n",
       " '##螃': 19140,\n",
       " '##董': 18926,\n",
       " '##许': 19444,\n",
       " '##鼠': 21019,\n",
       " '复': 1908,\n",
       " '蜱': 6061,\n",
       " 's5': 11161,\n",
       " '馋': 7669,\n",
       " '姣': 2004,\n",
       " '銬': 7073,\n",
       " '鄉': 6965,\n",
       " '##枕': 16416,\n",
       " '♥': 491,\n",
       " '芝': 5698,\n",
       " '##酱': 20053,\n",
       " '##熨': 17284,\n",
       " '##フ': 9383,\n",
       " '##喬': 14662,\n",
       " '##阳': 20402,\n",
       " 'ofo': 9574,\n",
       " 'event': 10992,\n",
       " '##コメント': 13301,\n",
       " '##归': 15552,\n",
       " '##推': 16029,\n",
       " '16': 8121,\n",
       " '悍': 2636,\n",
       " 'hit': 10295,\n",
       " '##w': 8220,\n",
       " '##竅': 18043,\n",
       " '鰓': 7813,\n",
       " '##tl': 12305,\n",
       " '##仨': 13867,\n",
       " '嘗': 1655,\n",
       " 'dha': 11015,\n",
       " '##89': 9402,\n",
       " '##泪': 16858,\n",
       " '幔': 2390,\n",
       " '貼': 6528,\n",
       " '##屉': 15293,\n",
       " '##諄': 19368,\n",
       " 'lumia': 11506,\n",
       " '##轮': 19819,\n",
       " '悬': 2647,\n",
       " '觅': 6227,\n",
       " '比': 3683,\n",
       " '##瞻': 17807,\n",
       " '##碣': 17875,\n",
       " '##駆': 20744,\n",
       " '★★★★★': 10349,\n",
       " 'plan': 12248,\n",
       " '紙': 5158,\n",
       " '##澜': 17130,\n",
       " '##代': 13864,\n",
       " '##№': 13518,\n",
       " '仰': 814,\n",
       " '##力': 14270,\n",
       " '炼': 4159,\n",
       " '##嬰': 15144,\n",
       " '##獣': 17416,\n",
       " '宫': 2151,\n",
       " '桡': 3438,\n",
       " '灿': 4136,\n",
       " '1975': 8974,\n",
       " '適': 6900,\n",
       " '##昭': 16277,\n",
       " '##萁': 18898,\n",
       " '##議': 19416,\n",
       " '##贷': 19644,\n",
       " '163': 8737,\n",
       " '愉': 2690,\n",
       " '瞻': 4750,\n",
       " 'ios': 8276,\n",
       " '忡': 2567,\n",
       " '##ference': 11419,\n",
       " '##ール': 9424,\n",
       " '##巍': 15388,\n",
       " 'butler': 11797,\n",
       " 'benz': 13065,\n",
       " '##↗': 13525,\n",
       " '萨': 5855,\n",
       " '##総': 18274,\n",
       " '##ker': 9013,\n",
       " '舎': 5651,\n",
       " '##ead': 12943,\n",
       " '（': 8020,\n",
       " '##珩': 17463,\n",
       " '##卜': 14358,\n",
       " '##缠': 18419,\n",
       " '##には': 10410,\n",
       " '##殁': 16706,\n",
       " '##傀': 14043,\n",
       " 'ф': 252,\n",
       " '红': 5273,\n",
       " '##yn': 10879,\n",
       " '##至': 18692,\n",
       " '##蟄': 19149,\n",
       " '##貂': 19560,\n",
       " '##刈': 14205,\n",
       " '##疊': 17597,\n",
       " '##邂': 19972,\n",
       " '##寓': 15228,\n",
       " '⑩': 414,\n",
       " '##喆': 14645,\n",
       " '##蔚': 18974,\n",
       " '##鄱': 20031,\n",
       " '##江': 16793,\n",
       " '難': 7432,\n",
       " '榔': 3524,\n",
       " '勁': 1233,\n",
       " '##li': 8636,\n",
       " 'hdr': 10465,\n",
       " '##浜': 16910,\n",
       " '་': 286,\n",
       " '泛': 3793,\n",
       " '蕲': 5942,\n",
       " '##lvin': 12745,\n",
       " 'there': 11136,\n",
       " '轻': 6768,\n",
       " '巴': 2349,\n",
       " '##俑': 13977,\n",
       " '##ø': 13364,\n",
       " '##孟': 15163,\n",
       " '##轲': 19822,\n",
       " '##槽': 16610,\n",
       " '##弘': 15530,\n",
       " '极': 3353,\n",
       " '##讥': 19429,\n",
       " '555': 11723,\n",
       " '诉': 6401,\n",
       " 'date': 11624,\n",
       " '##蝴': 19135,\n",
       " '遷': 6907,\n",
       " '咁': 1464,\n",
       " '鄲': 6975,\n",
       " '鋅': 7080,\n",
       " '##悶': 15710,\n",
       " '##豈': 19545,\n",
       " '##hip': 11489,\n",
       " '##郎': 20004,\n",
       " '121': 9247,\n",
       " '##観': 19276,\n",
       " '##ide': 9552,\n",
       " 'villa': 10806,\n",
       " 'm5': 12802,\n",
       " '債': 1002,\n",
       " 'wifi': 8306,\n",
       " 'after': 10100,\n",
       " '##oper': 11468,\n",
       " '##vd': 10754,\n",
       " '##冪': 14156,\n",
       " '珈': 4394,\n",
       " '##倡': 14013,\n",
       " 'gtx': 11069,\n",
       " '→': 370,\n",
       " '探': 2968,\n",
       " '頼': 7537,\n",
       " '##磐': 17889,\n",
       " '##缤': 18421,\n",
       " '##瓏': 17533,\n",
       " '乒': 728,\n",
       " '##楽': 16575,\n",
       " 'ヒ': 620,\n",
       " '[unused51]': 51,\n",
       " '褐': 6186,\n",
       " '##its': 11275,\n",
       " '轟': 6755,\n",
       " 'frigidaire': 12304,\n",
       " 'topapp': 10607,\n",
       " '##oon': 12343,\n",
       " '##lax': 12680,\n",
       " '##俊': 13973,\n",
       " '##槍': 16598,\n",
       " 'pwm': 11840,\n",
       " '##謄': 19391,\n",
       " '抗': 2834,\n",
       " '##骡': 20808,\n",
       " '##oy': 12532,\n",
       " '##诫': 19482,\n",
       " '##鬟': 20838,\n",
       " '##駭': 20750,\n",
       " '##hen': 11602,\n",
       " '##维': 18392,\n",
       " '與': 5645,\n",
       " '##mark': 9882,\n",
       " '偽': 984,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\",\n",
    "        \"有梦想谁都了不起\",\n",
    "        \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 32.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 363 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 877 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 266 ms\n",
      "Wall time: 80.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 738 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[39m=\u001b[39m slow_tokenizer(sen, return_offsets_mapping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2537\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2538\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2626\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2641\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2642\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2645\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2646\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2647\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2648\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2649\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2650\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2651\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2652\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2653\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2654\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2655\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2656\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2657\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2658\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2659\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2660\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2661\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2662\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2663\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2717\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2714\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2715\u001b[0m )\n\u001b[1;32m-> 2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2718\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2720\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2721\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2722\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2723\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2724\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2725\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2726\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2727\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2728\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2729\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2730\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2731\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2732\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2733\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2734\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2736\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils.py:641\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8357a6f05b4345baaffbb95f34fed2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuyao\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccfd3477c5c45e894350699ab1fffc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ce5c1fcbb4d10b64c6b766cf99d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='THUDM/chatglm-6b', vocab_size=130344, model_max_length=2048, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatglm_tokenizer\\\\tokenizer_config.json',\n",
       " 'chatglm_tokenizer\\\\special_tokens_map.json',\n",
       " 'chatglm_tokenizer\\\\ice_text.model',\n",
       " 'chatglm_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"chatglm_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
